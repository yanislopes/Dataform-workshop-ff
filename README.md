# Dataform-workshop-ff
Workshop Dataform - French Fragrance Analytics Pipeline

## Objectifs

- Comprendre les concepts clÃ©s de Dataform
- CrÃ©er des pipelines de transformation SQL (Full-Refresh et Incremental)
- Optimiser les coÃ»ts BigQuery avec partitioning et clustering
- GÃ©rer la qualitÃ© des donnÃ©es avec des assertions

## PrÃ©requis

- Compte Google Cloud Platform
- Ã‰diteur de code
- Git (Si besoin de cloner ce repo seulement)
- Gcloud CLI
- Node.js et npm
- Dataform CLI (npm install -g @dataform/cli)

## Setup & Hello World
**Option 1 : Suivre le workshop pas Ã  pas**
- Suivez les instructions ci-dessous

**Option 2 : Cloner le projet terminÃ©**
```bash
git clone https://github.com/yanislopes/Dataform-workshop-ff.git
cd Dataform-workshop-ff
npm install
```

### Step 1: Installer Dataform en local
#### **macOS**
```bash
# Installer gcloud CLI 
brew install --cask google-cloud-sdk
gcloud --version

# Installer Node.js (inclut npm) + vÃ©rifier les installations
brew install node
node --version
npm --version

# Installer Dataform CLI + vÃ©rifier l'installation
npm install -g @dataform/cli
dataform --version
```

#### **Windows**
- gcloud CLI : https://cloud.google.com/sdk/docs/install
- Node.js : https://nodejs.org/
- Dataform CLI : `npm install -g @dataform/cli` (dans PowerShell)

#### **Linux**
```bash
# gcloud CLI : https://cloud.google.com/sdk/docs/install
sudo apt install nodejs npm
npm install -g @dataform/cli
```

### Step 2 : Configuration GCP
#### **Authentification**
```bash
# Se connecter Ã  GCP
gcloud auth login

# Authentification pour les applications (utilisÃ© par Dataform CLI)
gcloud auth application-default login
```

#### **CrÃ©er et configurer un projet GCP**
**Via l'interface :**
1. Aller sur https://console.cloud.google.com
2. Cliquer sur le sÃ©lecteur de projet
3. **New Project**
4. **Project name**
5. CrÃ©er le projet

**Configurer gcloud avec ce projet :**
```bash
# Lister vos projets
gcloud projects list

# Configurer le projet par dÃ©faut
gcloud config set project VOTRE_PROJECT_ID
```

**Activer les APIs via l'interface :**
1. Aller dans la console GCP
2. **VÃ©rifier que vous Ãªtes dans le bon projet**
3. Dans la **barre de recherche**, taper : `Dataform` et sÃ©lectionner dataform marketplace
4. Activer l'API

**OU via CLI :**
```bash
# Activer BigQuery API
gcloud services enable bigquery.googleapis.com

# Activer Dataform API
gcloud services enable dataform.googleapis.com

# VÃ©rifier que les APIs sont activÃ©es
gcloud services list --enabled | grep -E 'bigquery|dataform'
```

### Step 3 : Initialiser le projet
```bash
# CrÃ©er et entrer dans le dossier du projet
mkdir dataform-workshop-ff
cd dataform-workshop-ff

# CrÃ©er la structure du hello_world
mkdir -p definitions/00_hello_world
mkdir -p includes

# CrÃ©er dataform.json
cat > dataform.json << 'EOF'
{
  "warehouse": "bigquery",
  "defaultDatabase": "VOTRE_PROJECT_ID",
  "defaultSchema": "DATASET_NAME",
  "assertionSchema": "dataform_assertions",
  "defaultLocation": "EU"
}
EOF

# CrÃ©er package.json
cat > package.json << 'EOF'
{
  "name": "dataform-workshop-ff",
  "version": "1.0.0",
  "dependencies": {
    "@dataform/core": "^3.0.0"
  }
}
EOF

# GÃ©nÃ©rer les credentials (fichier local, ne sera pas versionnÃ© dans Git)
cat > .df-credentials.json << 'EOF'
{
  "projectId": "VOTRE_PROJECT_ID",
  "location": "EU"
}
EOF

# CrÃ©er .gitignore
cat > .gitignore << 'EOF'
# Dataform
node_modules/
.df-credentials.json
EOF

# Installer les dÃ©pendances
npm install
```
PS : Le champ `"name"` dans package.json est l'identifiant npm du projet (vous pouvez le changer). On peur le voir avec la commande: npm list

### Step 4 : CrÃ©er le dataset source
```bash
# CrÃ©er le dataset pour les donnÃ©es brutes
bq mk --location=EU --dataset VOTRE_PROJECT_ID:raw_data
```
**OU via l'interface BigQuery :**
1. BigQuery â†’ Clic droit sur projet â†’ **Create dataset**
2. **Dataset ID** : `raw_data`
3. **Location** : `EU`
4. **Create**

### Step 5 : CrÃ©er la table source de test
```bash
# CrÃ©er la table hello_world dans le dataset raw_data
bq mk --table \
  VOTRE_PROJECT_ID:raw_data.hello_world \
  fragrance_id:INTEGER,brand_name:STRING,name:STRING,price_eur:FLOAT

# InsÃ©rer des donnÃ©es de test
bq query --use_legacy_sql=false "
INSERT INTO \`VOTRE_PROJECT_ID.raw_data.hello_world\` 
(fragrance_id, brand_name, name, price_eur)
VALUES 
  (1, 'Hello', 'world', 89.99)
"

# VÃ©rifier que les donnÃ©es sont bien insÃ©rÃ©es
bq query --use_legacy_sql=false \
  "SELECT * FROM \`VOTRE_PROJECT_ID.raw_data.hello_world\`"
```

**OU via l'interface BigQuery :**

1. Aller dans **BigQuery**
2. Ouvrir le dataset `raw_data`
3. Cliquer sur **CREATE TABLE**
4. **Table name** : `hello_world`
5. **Schema** :
   - `fragrance_id` : INTEGER
   - `brand_name` : STRING
   - `name` : STRING
   - `price_eur` : FLOAT
6. **Create table**
7. Dans l'Ã©diteur de requÃªtes, exÃ©cuter :
```sql
INSERT INTO `VOTRE_PROJECT_ID.raw_data.hello_world` 
(fragrance_id, brand_name, name, price_eur)
VALUES 
  (1, 'Hello', 'world', 89.99);

SELECT * FROM `VOTRE_PROJECT_ID.raw_data.hello_world`;
```

### Step 6 : CrÃ©er la source et la transformation hello_world
#### **A. CrÃ©er la structure des dossiers**
```bash
mkdir -p definitions/00_hello_world/sources
mkdir -p definitions/00_hello_world/stage
mkdir -p definitions/00_hello_world/output
```

#### **B. DÃ©clarer la source**
CrÃ©er le fichier `definitions/00_hello_world/sources/raw_data.sqlx` :
```sql
config {
  type: "declaration",
  database: "VOTRE_PROJECT_ID",
  schema: "raw_data",
  name: "hello_world"
}
```
**Explication :** 
- DÃ©clare la table source existante dans BigQuery (`raw_data.hello_world`)
- Permet d'utiliser `ref()` pour rÃ©fÃ©rencer cette table
- Tag `hello_world` pour exÃ©cuter tout le pipeline `hello_world` (pas obligatoire)

#### **C. CrÃ©er la transformation stage**

CrÃ©er le fichier `definitions/00_hello_world/stage/stage_hello_world.sqlx` :
```sql
config {
  type: "table",
  database: "VOTRE_PROJECT_ID",
  schema: "stage_congrat_data",
  name: "stage_hello_world",
  description: "Temp French Fragrance Hello World",
  tags: ["hello_world"]
}

SELECT 
  fragrance_id,
  brand_name,
  name,
  price_eur,
  'Made in Paris, France' as origin,
  CURRENT_TIMESTAMP() as technical_date
FROM ${ref("raw_data", "hello_world")}
```

**Explication :**
- CrÃ©e une table temporaire dans `stage_congrat_data`
- Ajoute les colonnes `origin` et `technical_date`
- RÃ©fÃ©rence la source via `${ref("raw_data", "hello_world")}`

#### **D. CrÃ©er la transformation output**

CrÃ©er le fichier `definitions/00_hello_world/output/hello_world.sqlx` :
```sql
config {
  type: "table",
  database: "VOTRE_PROJECT_ID",
  schema: "congrat_data",
  description: "French Fragrance Hello World",
  tags: ["hello_world"],
  dependencies: ["stage_hello_world"]
}

SELECT *
FROM ${ref("stage_congrat_data", "stage_hello_world")}
```

**Explication :**
- Le nom de la table sera `hello_world` (nom du fichier .sqlx)
- CrÃ©e la table finale dans `congrat_data`
- RÃ©fÃ©rence le stage via `${ref("stage_congrat_data", "stage_hello_world")}`
- `dependencies` assure que le stage s'exÃ©cute avant l'output

**Architecture du pipeline :**
```
raw_data.hello_world (source)
    â†“
stage_congrat_data.stage_hello_world (transformation)
    â†“
congrat_data.hello_world (output final)
```

### Step 7 : Compiler et exÃ©cuter le pipeline
#### **A. Compiler le projet**
```bash
# Dans le dossier dataform-workshop-ff
dataform compile
```

#### **B. ExÃ©cuter le pipeline hello_world**
```bash
# ExÃ©cuter uniquement le tag hello_world
dataform run --tags hello_world
```

#### **C. VÃ©rifier les rÃ©sultats dans BigQuery**
1. Aller dans **BigQuery**
2. Naviguer vers `congrat_data` puis dans `hello_world`
3. Cliquer sur **Preview**
4. VÃ©rifier que les donnÃ©es contiennent `Made in Paris, France` et `technical_date`

## Bonus : Utiliser l'interface Dataform GCP (dÃ©mo)
**Optionnel** : Juste pour voir l'interface, sans connexion au projet local

1. CrÃ©er un repository Dataform dans GCP
2. CrÃ©er un workspace
3. Coder directement dans l'interface
4. Compiler et exÃ©cuter
5. Visualiser le DAG









## ðŸ“š Structure du projet
```
dataform-workshop-ff/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE (MIT)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ dataform.json           
â”œâ”€â”€ package.json 
â”œâ”€â”€ definitions/
â”‚   â”œâ”€â”€ 00_hello_world/
â”‚   â”œâ”€â”€ 01_workshop_full_refresh/
â”‚   â””â”€â”€ 02_workshop_incremental/
â””â”€â”€ includes/
```

**Pas de synchronisation avec le projet local, c'est juste pour la dÃ©mo !**

## ðŸ“– Workshops

### Workshop 1 : Pipeline Full-Refresh (1h30)
- **ScÃ©nario** : Reporting Hebdomadaire des Ventes
- **Focus** : Fondamentaux Dataform, architecture en layers
- **StratÃ©gie** : Full-refresh (tables recalculÃ©es complÃ¨tement)

### Workshop 2 : Pipeline Incremental (1h30)
- **ScÃ©nario** : Analytics Temps RÃ©el des OpÃ©rations (2 ans d'historique)
- **Focus** : Optimisation coÃ»ts, performances, gros volumes
- **StratÃ©gie** : Incremental (mise Ã  jour partielle)

